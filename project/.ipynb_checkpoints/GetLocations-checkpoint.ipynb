{
 "metadata": {
  "name": "",
  "signature": "sha256:7b7e4b9041e6d612bc584524125e54c3777e03e2b23883f11f72f92859087461"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from twython import Twython\n",
      "import json\n",
      "import datetime \n",
      "\n",
      "##codes to access twitter API. \n",
      "APP_KEY = \"9jlDDVPYsigPe8Ao9UbnqjQ7Q\"\n",
      "APP_SECRET = \"jVHAOFrKHfpBOaJzH2K0F0ZRgB3bz5rVdlqoGQAup5tXaaspGT\"\n",
      "OAUTH_TOKEN = \"102152165-Sdv2opvPufyotWasY7U2C5G1ofxrckFvDtKKvTpx\"\n",
      "OAUTH_TOKEN_SECRET = \"FoBl3PW21BZoUeYi4WDbcUHuMC4YTAL5ZOvctMVB3eAsl\"\n",
      "\n",
      "##initiating Twython object\n",
      "twitter = Twython(APP_KEY, APP_SECRET, OAUTH_TOKEN, OAUTH_TOKEN_SECRET)\n",
      "\n",
      "def get_city_id(latitude, longitude):\n",
      "    search_results = twitter.reverse_geocode(lat=latitude, long=longitude, granularity=\"city\")\n",
      "\n",
      "    #\n",
      "\n",
      "    output_file = 'places.csv' \n",
      "    target = open(output_file, 'a')\n",
      "\n",
      "    for result in search_results['result']['places']:\n",
      "            place_type = result['place_type']\n",
      "            if place_type == \"city\":\n",
      "                city = result['full_name']\n",
      "                place_id = result['id']\n",
      "                lat_c = result['centroid'][0]\n",
      "                long_c = result['centroid'][1]\n",
      "                target.write(city + '\\t' + place_id + '\\t' + str(lat_c) + '\\t' + str(long_c))\n",
      "                target.write('\\n')\n",
      "        \n",
      "    target.close()\n",
      "\n",
      "get_city_id(29.762778, -95.383056)\n",
      "get_city_id(29.416667, -98.5)\n",
      "get_city_id(32.775833, -96.796667)\n",
      "get_city_id(30.266667, -97.75)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import requests\n",
      "\n",
      "r = requests.post('http://text-processing.com/api/sentiment/', data = {'text':'great movie'})\n",
      "print r\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<Response [200]>\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print r.text\n",
      "print type(r)\n",
      "print r.json()['label']\n",
      "print r.json()['probability']['neg']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{\"probability\": {\"neg\": 0.28585169214759543, \"neutral\": 0.10852709088757791, \"pos\": 0.71414830785240457}, \"label\": \"pos\"}\n",
        "<class 'requests.models.Response'>\n",
        "pos\n",
        "0.285851692148\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import csv\n",
      "import requests\n",
      "\n",
      "## Open the tweets file and gets the sentiment analysis, counting the number of pos, neg and neutral\n",
      "\n",
      "with open('tweets/Houston/@real_donald_trump.csv', 'r') as infile, open('tweets/Houston/@real_donald_trump_sentiment.csv', 'wb') as outfile:\n",
      "    reader = csv.DictReader(infile, delimiter=';')\n",
      "    fieldnames = reader.fieldnames + ['label'] + ['probability']\n",
      "    writer = csv.DictWriter(outfile, fieldnames)\n",
      "    writer.writeheader()\n",
      "    pos_count = 0\n",
      "    neg_count = 0\n",
      "    net_count = 0\n",
      "    for tweet in reader:\n",
      "        r = requests.post('http://text-processing.com/api/sentiment/', data = {'text':tweet['text']})\n",
      "        #tweet['label'] = r.json()['label']\n",
      "        #tweet['probability'] = r.json()['probability']\n",
      "        writer.writerow(dict(tweet, label = r.json()['label'], probability = r.json()['probability']))\n",
      "        \n",
      "        if r.json()['label'] == 'pos':\n",
      "            pos_count += 1\n",
      "        elif r.json()['label'] == 'neg':\n",
      "            neg_count += 1\n",
      "        else:\n",
      "            net_count += 1  \n",
      "print pos_count\n",
      "print neg_count\n",
      "print net_count"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print pos_count\n",
      "print neg_count\n",
      "print net_count\n",
      "\n",
      "#output_file = 'places.csv' \n",
      "#target = open(output_file, 'a')\n",
      "\n",
      "#for place in target:\n",
      "#    place_type = result['place_type']\n",
      "#    if place_type == \"Houston, TX\":\n",
      "#        target.write(city + '\\t' + place_id + '\\t' + str(lat_c) + '\\t' + str(long_c))\n",
      "#        target.write('\\n')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "183\n",
        "192\n",
        "374\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import zipfile, urllib2\n",
      "response = urllib2.urlopen('http://www2.census.gov/geo/tiger/GENZ2015/shp/cb_2015_us_ua10_500k.zip')\n",
      "zipcontent= response.read()\n",
      "with open(\"cb_2015_us_ua10_500k.zip\", 'w') as f:\n",
      "    f.write(zipcontent)\n",
      "\n",
      "zip_ref = zipfile.ZipFile(\"cb_2015_us_ua10_500k.zip\", 'r')\n",
      "zip_ref.extractall(\"us_cities\")\n",
      "zip_ref.close()\n",
      "\n",
      "response = urllib2.urlopen('http://www2.census.gov/geo/tiger/GENZ2015/shp/cb_2015_us_state_500k.zip')\n",
      "zipcontent= response.read()\n",
      "with open(\"cb_2015_us_state_500k.zip\", 'w') as f:\n",
      "    f.write(zipcontent)\n",
      "\n",
      "zip_ref = zipfile.ZipFile(\"cb_2015_us_state_500k.zip\", 'r')\n",
      "zip_ref.extractall(\"us_states\")\n",
      "zip_ref.close()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import shapefile\n",
      "from json import dumps\n",
      "\n",
      "reader = shapefile.Reader(\"us_cities/cb_2015_us_ua10_500k.shp\")\n",
      "fields = reader.fields[1:]\n",
      "field_names = [field[0] for field in fields]\n",
      "buffer = []\n",
      "places = open('places.csv', 'r')\n",
      "for line in places:\n",
      "    attr = line.split(\";\")\n",
      "    city = attr[0]\n",
      "    for sr in reader.shapeRecords():\n",
      "        atr = dict(zip(field_names, sr.record))\n",
      "        geom = sr.shape.__geo_interface__\n",
      "        if atr['NAME10'] == city:\n",
      "            buffer.append(dict(type=\"Feature\",\n",
      "                           geometry=geom, properties=atr)) \n",
      "places.close()   \n",
      "## write the GeoJSON file (urban areas)\n",
      "geojson = open(\"us_cities.json\", \"w\")\n",
      "geojson.write(dumps({\"type\": \"FeatureCollection\",\n",
      "                     \"features\": buffer}, indent=2) + \"\\n\")\n",
      "geojson.close()\n",
      "\n",
      "# read the shapefile (states)\n",
      "reader = shapefile.Reader(\"us_states/cb_2015_us_state_500k.shp\")\n",
      "fields = reader.fields[1:]\n",
      "field_names = [field[0] for field in fields]\n",
      "buffer = []\n",
      "for sr in reader.shapeRecords():\n",
      "    atr = dict(zip(field_names, sr.record))\n",
      "    geom = sr.shape.__geo_interface__\n",
      "    if atr['NAME'] == 'Texas':\n",
      "        buffer.append(dict(type=\"Feature\",\n",
      "                           geometry=geom, properties=atr)) \n",
      "   \n",
      "   # write the GeoJSON file\n",
      "geojson = open(\"us_states.json\", \"w\")\n",
      "geojson.write(dumps({\"type\": \"FeatureCollection\",\n",
      "                     \"features\": buffer}, indent=2) + \"\\n\")\n",
      "geojson.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import folium, os\n",
      "\n",
      "\n",
      "texas = os.path.join(\"us_states.json\")\n",
      "houston = os.path.join(\"us_cities.json\")\n",
      "map_texas = folium.Map(location=[29.8384948, -95.4708095532],tiles=\"Mapbox Bright\", zoom_start=8)\n",
      "map_texas.choropleth(geo_path=texas, fill_color='red')\n",
      "map_texas.choropleth(geo_path=houston)\n",
      "map_texas.save('texas.html')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}